{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;background-color:rgb(255, 108, 0);padding-top:1em;padding-bottom:0.7em;padding-left:1em;\">Optical flow-based segmentation of moving objects for mobile robot navigation using pre-trained deep learning models</h1>\n",
    "<hr>\n",
    "\n",
    "<h2>Auxiliary materials</h2>\n",
    "\n",
    "This notebook contains some helper functions to implement the compound training strategy for binary image segmentation that is introduced in our paper.\n",
    "<br>\n",
    "The training is based on the combination of two loss functions:\n",
    "\n",
    "- The Dice loss and\n",
    "- The Cross Entropy loss\n",
    "\n",
    "<h3>Dice loss</h3>\n",
    "\n",
    "The Dice loss is based on the Sørensen–Dice coefficient (Dice coefficient), with which the overlap between two sets, A and B can be computed like:\n",
    "\n",
    "$$\n",
    "Dice = \\frac{2|A\\cap B|}{|A|+|B|}.\n",
    "$$\n",
    "\n",
    "In case of binary segmentation, A and B are the predicted and groud-truth segmentation maps respectively. If the pixels of the segmentaion maps are not interpreted as binary values, but rather as probabilities, the Soft Dice coefficient can be used:\n",
    "\n",
    "$$\n",
    "Soft Dice = \\frac{2\\sum\\limits_{i=1}^Ny_ip_i}{\\sum\\limits_{i=1}^Np_i+\\sum\\limits_{i=1}^Ny_i},\n",
    "$$\n",
    "\n",
    "where N is the number of pixels in the output segmentation map,\n",
    "<br>\n",
    "$y_i$ is the correct label for the $i^{th}$ output pixel (0 or 1) and\n",
    "<br>\n",
    "$p_i$ is the predicted probability of the $i^{th}$ pixel having a correct label of 1.\n",
    "\n",
    "From the Soft Dice coefficient the Dice loss can be formulated like:\n",
    "\n",
    "$$\n",
    " L_{SD}=1-Soft Dice.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The following code contains implementation of the Soft Dice coefficient and the Dice loss:\n",
    "<br>\n",
    "(Please note that we assume sigmoid activation function in the output layer. If an other activation function is used in the output layer the following code should be modified accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(labels, logits):\n",
    "    '''This function computes the Soft Sørensen–Dice coefficient.\n",
    "    \n",
    "    Args:\n",
    "        labels - (batch, height x width) shaped tensor of the label maps for the binary segmentation problem\n",
    "        logits - (batch, height x width) shaped tensor of the logits\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape () containing the Soft Sørensen–Dice coefficient for labels and logits\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('Dice_Coefficient'):\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        predictions = tf.nn.sigmoid(tf.cast(logits, tf.float32))\n",
    "        num = tf.cast(tf.reduce_sum(2*labels*predictions, axis=1), tf.float32) + tf.constant(0.01)\n",
    "        den = tf.cast(tf.reduce_sum(labels, axis=1)+tf.reduce_sum(predictions, axis=1), tf.float32) + tf.constant(0.01)\n",
    "        dice_coeff = tf.reduce_mean(num/den, name='dice_coeff')\n",
    "    return dice_coeff\n",
    "\n",
    "def dice_loss(labels, logits):\n",
    "    '''This function computes the Dice loss for binary classification\n",
    "    \n",
    "    Args:\n",
    "        labels - (batch, height x width) shaped tensor of the label maps for the binary segmentation problem\n",
    "        logits - (batch, height x width) shaped tensor of the logits\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape () containing the Dice loss for labels and logits. This value is bound between 0 and 1.\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('Dice_Loss'):\n",
    "        dice_loss = tf.identity(1-dice_coeff(labels, logits), name='dice_loss')\n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compound loss</h3>\n",
    "\n",
    "The compund loss is calculated from the linear combination of the Dice loss and the Cross Entropy loss:\n",
    "\n",
    "$$\n",
    "L = (1-\\alpha)L_{CE}+\\alpha L_{SD},\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the coefficient for controlling how much the Dice loss and the Cross Entropy loss contribute to the compound loss,\n",
    "<br>\n",
    "$L_{CE}$ is the Cross Entropy loss and\n",
    "<br>\n",
    "$L_{SD}$ is the Dice loss.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The following function implements the compund loss (for the Cross Entropy loss we used the built-in function from TensorFlow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_loss(labels, logits):\n",
    "    '''This function computes the compound loss for the binary classification problem\n",
    "    \n",
    "    Args:\n",
    "        labels - (batch, height x width) shaped tensor of the label maps for the binary segmentation problem\n",
    "        logits - (batch, height x width) shaped tensor of the logits\n",
    "                \n",
    "    Returns:\n",
    "            Tuple of a tensor of shape () containing the compound loss for labels and logits for a certain alpha and\n",
    "            a tensor to set the value of alpha during training with the feed_dict parameter of the tf.Session().run()\n",
    "            call. Alpha is a scalar to control how much the Dice loss and the Cross Entropy loss contribute to the\n",
    "            compound loss (default value is 0, the value for alpha should be in the [0,1] interval). A value of 0 means\n",
    "            that the compound loss is entirely computed from the Cross Entropy loss and a value of 1 means that the\n",
    "            compound loss is entirely computed from the Dice loss. Please note that setting a value for alpha that is\n",
    "            smaller than 0 or greater than 1 does not raise an error, but the value gets clipped to either 0 or 1\n",
    "            respectively.\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('Compound_Loss'):\n",
    "        alpha = tf.constant(0, dtype=tf.float32, name='alpha')\n",
    "        loss_weight = tf.clip_by_value(alpha, 0, 1, name='loss_weight')\n",
    "        compound_loss = (1-loss_weight)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(labels,tf.float32),logits=tf.cast(logits,tf.float32)))+loss_weight*dice_loss(labels,logits)\n",
    "    return (compound_loss, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Usage</h3>\n",
    "\n",
    "The code bellow shows an example of how the compound loss function can be used in a training loop, if the number of iterations is known a priori. We found that setting the value of alpha according to\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{\\left(\\dfrac{j}{n}\\right)^4}{1.6},\n",
    "$$\n",
    "\n",
    "where $n$ is the number of iterations and $j$ is the current iteration number, is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training data or create a data stream and define the tensor for the ground-truth labels:\n",
    "\n",
    "#...\n",
    "labels = #...\n",
    "\n",
    "\n",
    "# Define a model and collect its outputs, without the sigmoid activation, in logits:\n",
    "\n",
    "#...\n",
    "logits = #...\n",
    "\n",
    "\n",
    "# Define the compound loss and the training step operation (minimize the compound loss):\n",
    "\n",
    "cl, alpha = compound_loss(labels, logits)\n",
    "optimizer = #...\n",
    "train_op = optimizer.minimize(cl)\n",
    "\n",
    "\n",
    "# Number of training iterations:\n",
    "\n",
    "n = #...\n",
    "\n",
    "\n",
    "# Training loop:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initilaization operations:\n",
    "    \n",
    "    #...\n",
    "    \n",
    "    \n",
    "    # Training:\n",
    "    \n",
    "    for j in range(n):\n",
    "        sess.run(train_op, feed_dict={alpha: ((j/n)**4)/1.6})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
